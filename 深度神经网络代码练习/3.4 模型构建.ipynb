{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MLP,self).__init__(**kwargs)\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        self.act = nn.ReLU()\n",
    "        self.output = nn.Linear(256,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        o = self.act(self.hidden(x))\n",
    "        return self.output(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand(2,784)\n",
    "net = MLP()\n",
    "print(net)\n",
    "net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self,**kwargs):\n",
    "        super(MyLayer,self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x - x.mean()   \n",
    "\n",
    "layer = MyLayer()\n",
    "x = torch.tensor([1,2,3,4,5],dtype = torch.float)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyListDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyListDense,self).__init__()\n",
    "        self.params = nn.ParameterList([nn.Parameter(torch.randn(4,4)) for i in range(3)])\n",
    "        self.params.append(nn.Parameter(torch.randn(4,1)))\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(len(self.params)):\n",
    "            x = torch.mm(x,self.params[i])\n",
    "        return x\n",
    "net = MyListDense()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDictDense(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyDictDense,self).__init__()\n",
    "        self.params = nn.ParameterDict({\n",
    "            'linear1' : nn.Parameter(torch.randn(4,4)),\n",
    "            'linear2' : nn.Parameter(torch.randn(4,1))\n",
    "        })\n",
    "        self.params.update({\n",
    "            'linear3':nn.Parameter(torch.randn(4,2))\n",
    "        })\n",
    "    \n",
    "    def forward(self,x,choice = 'linear1'):\n",
    "        return torch.mm(x,self.params[choice])\n",
    "net = MyDictDense()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def corr2d(X,K):\n",
    "    h,w = K.shape\n",
    "    X,K = X.float(),K.float()\n",
    "    Y = torch.zeros((X.shape[0] - h +1,X.shape[1] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        for j in range(Y.shape[1]):\n",
    "            Y[i,j] = (X[i:i + h,j:j + w] * K).sum()\n",
    "    return Y\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self,kernel_size):\n",
    "        super(Conv2D,self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(kernel_size,kernel_size))\n",
    "        self.bias = nn.Parameter(torch.randn(1))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return corr2d(x,self.weight) + self.bias\n",
    "    \n",
    "X = torch.tensor([[1, 1, 1, 1, 1] for i in range(5)])\n",
    "conv2D = Conv2D(3)\n",
    "output = conv2D(X)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def comp_conv2d(conv2d,X):\n",
    "    X = X.view((1,1)+X.shape)\n",
    "    Y = conv2d(X)\n",
    "    return Y.view(Y.shape[2:])\n",
    "\n",
    "\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels = 1,out_channels = 1,kernel_size = 3,padding = 1)\n",
    "X = torch.rand(8,8)\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(in_channels = 1, out_channels = 1,kernel_size = (5,3),padding = (2,1))\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv2d = nn.Conv2d(1,1,kernel_size = (3,5),padding = (0,1),stride=(3,4))\n",
    "comp_conv2d(conv2d,X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def pool2d(X, pool_size, mode='max'):\n",
    "    p_h, p_w = pool_size\n",
    "    Y = torch.zeros(X.shape[0] - p_h + 1, X.shape[1] - p_w + 1)\n",
    "    \n",
    "    for i in range(Y.shape[0]):  # Adjusted range\n",
    "        for j in range(Y.shape[1]):  # Adjusted range\n",
    "            if mode == 'max':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].max()\n",
    "            elif mode == 'avg':\n",
    "                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()\n",
    "    return Y\n",
    "\n",
    "X = torch.arange(9, dtype=torch.float).reshape(3, 3)\n",
    "pool2d(X, (2, 2), mode='avg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5)\n",
    "        self.conv2 = nn.Conv2d(6,16,5)\n",
    "\n",
    "        self.fc1 = nn.Linear(16*5*5,120)\n",
    "        self.fc2 = nn.Linear(120,84)\n",
    "        self.fc3 = nn.Linear(84,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)),(2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)),(2,2))\n",
    "        x = x.view(-1,int(x.numel()/x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "x = torch.rand(1,1,32,32)\n",
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.0330, 0.0777, 0.0170, 0.0000, 0.0644, 0.0000, 0.1316, 0.0000,\n",
      "         0.0653]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1,96,11,4),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "            nn.Conv2d(96,256,5,1,2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2),\n",
    "            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。\n",
    "            # 前两个卷积层后不使用池化层来减小输入的高和宽\n",
    "            nn.Conv2d(256,384,3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384,384,3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384,256,3,1,1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3,2)\n",
    "        )\n",
    "\n",
    "        # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*5*5,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(4096,10)\n",
    "        )\n",
    "    \n",
    "    def forward(self,img):\n",
    "        feature = self.conv(img)\n",
    "        output = self.fc(feature.view(img.shape[0],-1))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU()\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = AlexNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h_pytorch",
   "language": "python",
   "name": "h_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
