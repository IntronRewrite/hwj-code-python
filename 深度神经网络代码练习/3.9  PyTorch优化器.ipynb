{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.9 PyTorch优化器\n",
    "\n",
    "深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，只不过这个最优解是一个矩阵，而如何快速求得这个最优解是深度学习研究的一个重点，以经典的resnet-50为例，它大约有2000万个系数需要进行计算，那么我们如何计算出这么多系数，有以下两种方法：\n",
    "\n",
    "1. 第一种是直接暴力穷举一遍参数，这种方法从理论上行得通，但是实施上可能性基本为0，因为参数量过于庞大。\n",
    "2. 为了使求解参数过程更快，人们提出了第二种办法，即BP+优化器逼近求解。\n",
    "\n",
    "因此，优化器是根据网络反向传播的梯度信息来更新网络的参数，以起到降低loss函数计算值，使得模型输出更加接近真实标签。\n",
    "\n",
    "经过本节的学习，你将收获：\n",
    "\n",
    "- 了解PyTorch的优化器\n",
    "- 学会使用PyTorch提供的优化器进行优化\n",
    "- 优化器的属性和构造\n",
    "- 优化器的对比\n",
    "\n",
    "\n",
    "## 3.9.1 PyTorch提供的优化器\n",
    "\n",
    "PyTorch很人性化的给我们提供了一个优化器的库`torch.optim`，在这里面提供了多种优化器。\n",
    "\n",
    "+ torch.optim.SGD \n",
    "+ torch.optim.ASGD\n",
    "+ torch.optim.Adadelta\n",
    "+ torch.optim.Adagrad\n",
    "+ torch.optim.Adam\n",
    "+ torch.optim.AdamW\n",
    "+ torch.optim.Adamax\n",
    "+ torch.optim.RAdam\n",
    "+ torch.optim.NAdam\n",
    "+ torch.optim.SparseAdam\n",
    "+ torch.optim.LBFGS\n",
    "+ torch.optim.RMSprop\n",
    "+ torch.optim.Rprop\n",
    "\n",
    "而以上这些优化算法均继承于`Optimizer`，下面我们先来看下所有优化器的基类`Optimizer`。定义如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(object):\n",
    "    def __init__(self, params, defaults):        \n",
    "        self.defaults = defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.param_groups = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Optimizer`有三个属性：**\n",
    "\n",
    "+ `defaults`：存储的是优化器的超参数，例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`state`：参数的缓存，例子如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "defaultdict(<class 'dict'>, {tensor([[ 0.3864, -0.0131],\n",
    "        [-0.1911, -0.4511]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Optimizer`还有以下的方法：**\n",
    "\n",
    "+ `zero_grad()`：清空所管理参数的梯度，PyTorch的特性是张量的梯度不自动清零，因此每次反向传播后都需要清空梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_grad(self, set_to_none: bool = False):\n",
    "    for group in self.param_groups:\n",
    "        for p in group['params']:\n",
    "            if p.grad is not None:  #梯度不为空\n",
    "                if set_to_none: \n",
    "                    p.grad = None\n",
    "                else:\n",
    "                    if p.grad.grad_fn is not None:\n",
    "                        p.grad.detach_()\n",
    "                    else:\n",
    "                        p.grad.requires_grad_(False)\n",
    "                    p.grad.zero_()# 梯度设置为0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`step()`：执行一步梯度更新，参数更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(self, closure): \n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`add_param_group()`：添加参数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_param_group(self, param_group):\n",
    "    assert isinstance(param_group, dict), \"param group must be a dict\"\n",
    "# 检查类型是否为tensor\n",
    "    params = param_group['params']\n",
    "    if isinstance(params, torch.Tensor):\n",
    "        param_group['params'] = [params]\n",
    "    elif isinstance(params, set):\n",
    "        raise TypeError('optimizer parameters need to be organized in ordered collections, but '\n",
    "                        'the ordering of tensors in sets will change between runs. Please use a list instead.')\n",
    "    else:\n",
    "        param_group['params'] = list(params)\n",
    "    for param in param_group['params']:\n",
    "        if not isinstance(param, torch.Tensor):\n",
    "            raise TypeError(\"optimizer can only optimize Tensors, \"\n",
    "                            \"but one of the params is \" + torch.typename(param))\n",
    "        if not param.is_leaf:\n",
    "            raise ValueError(\"can't optimize a non-leaf Tensor\")\n",
    "\n",
    "    for name, default in self.defaults.items():\n",
    "        if default is required and name not in param_group:\n",
    "            raise ValueError(\"parameter group didn't specify a value of required optimization parameter \" +\n",
    "                             name)\n",
    "        else:\n",
    "            param_group.setdefault(name, default)\n",
    "\n",
    "    params = param_group['params']\n",
    "    if len(params) != len(set(params)):\n",
    "        warnings.warn(\"optimizer contains a parameter group with duplicate parameters; \"\n",
    "                      \"in future, this will cause an error; \"\n",
    "                      \"see github.com/PyTorch/PyTorch/issues/40967 for more information\", stacklevel=3)\n",
    "# 上面好像都在进行一些类的检测，报Warning和Error\n",
    "    param_set = set()\n",
    "    for group in self.param_groups:\n",
    "        param_set.update(set(group['params']))\n",
    "\n",
    "    if not param_set.isdisjoint(set(param_group['params'])):\n",
    "        raise ValueError(\"some parameters appear in more than one parameter group\")\n",
    "# 添加参数\n",
    "    self.param_groups.append(param_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_state_dict()` ：加载状态参数字典，可以用来进行模型的断点续训练，继续上次的参数进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_state_dict(self, state_dict):\n",
    "    r\"\"\"Loads the optimizer state.\n",
    "\n",
    "    Arguments:\n",
    "        state_dict (dict): optimizer state. Should be an object returned\n",
    "            from a call to :meth:`state_dict`.\n",
    "    \"\"\"\n",
    "    # deepcopy, to be consistent with module API\n",
    "    state_dict = deepcopy(state_dict)\n",
    "    # Validate the state_dict\n",
    "    groups = self.param_groups\n",
    "    saved_groups = state_dict['param_groups']\n",
    "\n",
    "    if len(groups) != len(saved_groups):\n",
    "        raise ValueError(\"loaded state dict has a different number of \"\n",
    "                         \"parameter groups\")\n",
    "    param_lens = (len(g['params']) for g in groups)\n",
    "    saved_lens = (len(g['params']) for g in saved_groups)\n",
    "    if any(p_len != s_len for p_len, s_len in zip(param_lens, saved_lens)):\n",
    "        raise ValueError(\"loaded state dict contains a parameter group \"\n",
    "                         \"that doesn't match the size of optimizer's group\")\n",
    "\n",
    "    # Update the state\n",
    "    id_map = {old_id: p for old_id, p in\n",
    "              zip(chain.from_iterable((g['params'] for g in saved_groups)),\n",
    "                  chain.from_iterable((g['params'] for g in groups)))}\n",
    "\n",
    "    def cast(param, value):\n",
    "        r\"\"\"Make a deep copy of value, casting all tensors to device of param.\"\"\"\n",
    "\n",
    "\n",
    "    # Copy state assigned to params (and cast tensors to appropriate types).\n",
    "    # State that is not assigned to params is copied as is (needed for\n",
    "    # backward compatibility).\n",
    "    state = defaultdict(dict)\n",
    "    for k, v in state_dict['state'].items():\n",
    "        if k in id_map:\n",
    "            param = id_map[k]\n",
    "            state[param] = cast(param, v)\n",
    "        else:\n",
    "            state[k] = v\n",
    "\n",
    "    # Update parameter groups, setting their 'params' value\n",
    "    def update_group(group, new_group):\n",
    "       ...\n",
    "    param_groups = [\n",
    "        update_group(g, ng) for g, ng in zip(groups, saved_groups)]\n",
    "    self.__setstate__({'state': state, 'param_groups': param_groups})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`state_dict()`：获取优化器当前状态信息字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_dict(self):\n",
    "    r\"\"\"Returns the state of the optimizer as a :class:`dict`.\n",
    "\n",
    "    It contains two entries:\n",
    "\n",
    "    * state - a dict holding current optimization state. Its content\n",
    "        differs between optimizer classes.\n",
    "    * param_groups - a dict containing all parameter groups\n",
    "    \"\"\"\n",
    "    # Save order indices instead of Tensors\n",
    "    param_mappings = {}\n",
    "    start_index = 0\n",
    "\n",
    "    def pack_group(group):\n",
    "    param_groups = [pack_group(g) for g in self.param_groups]\n",
    "    # Remap state to use order indices as keys\n",
    "    packed_state = {(param_mappings[id(k)] if isinstance(k, torch.Tensor) else k): v\n",
    "                    for k, v in self.state.items()}\n",
    "    return {\n",
    "        'state': packed_state,\n",
    "        'param_groups': param_groups,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.9.2 实际操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# 设置权重，服从正态分布  --> 2 x 2\n",
    "weight = torch.randn((2, 2), requires_grad=True)\n",
    "# 设置梯度为全1矩阵  --> 2 x 2\n",
    "weight.grad = torch.ones((2, 2))\n",
    "# 输出现有的weight和data\n",
    "print(\"The data of weight before step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight before step:\\n{}\".format(weight.grad))\n",
    "# 实例化优化器\n",
    "optimizer = torch.optim.SGD([weight], lr=0.1, momentum=0.9)\n",
    "# 进行一步操作\n",
    "optimizer.step()\n",
    "# 查看进行一步后的值，梯度\n",
    "print(\"The data of weight after step:\\n{}\".format(weight.data))\n",
    "print(\"The grad of weight after step:\\n{}\".format(weight.grad))\n",
    "# 权重清零\n",
    "optimizer.zero_grad()\n",
    "# 检验权重是否为0\n",
    "print(\"The grad of weight after optimizer.zero_grad():\\n{}\".format(weight.grad))\n",
    "# 输出参数\n",
    "print(\"optimizer.params_group is \\n{}\".format(optimizer.param_groups))\n",
    "# 查看参数位置，optimizer和weight的位置一样，我觉得这里可以参考Python是基于值管理\n",
    "print(\"weight in optimizer:{}\\nweight in weight:{}\\n\".format(id(optimizer.param_groups[0]['params'][0]), id(weight)))\n",
    "# 添加参数：weight2\n",
    "weight2 = torch.randn((3, 3), requires_grad=True)\n",
    "optimizer.add_param_group({\"params\": weight2, 'lr': 0.0001, 'nesterov': True})\n",
    "# 查看现有的参数\n",
    "print(\"optimizer.param_groups is\\n{}\".format(optimizer.param_groups))\n",
    "# 查看当前状态信息\n",
    "opt_state_dict = optimizer.state_dict()\n",
    "print(\"state_dict before step:\\n\", opt_state_dict)\n",
    "# 进行5次step操作\n",
    "for _ in range(50):\n",
    "    optimizer.step()\n",
    "# 输出现有状态信息\n",
    "print(\"state_dict after step:\\n\", optimizer.state_dict())\n",
    "# 保存参数信息\n",
    "torch.save(optimizer.state_dict(),os.path.join(r\"D:\\pythonProject\\Attention_Unet\", \"optimizer_state_dict.pkl\"))\n",
    "print(\"----------done-----------\")\n",
    "# 加载参数信息\n",
    "state_dict = torch.load(r\"D:\\pythonProject\\Attention_Unet\\optimizer_state_dict.pkl\") # 需要修改为你自己的路径\n",
    "optimizer.load_state_dict(state_dict)\n",
    "print(\"load state_dict successfully\\n{}\".format(state_dict))\n",
    "# 输出最后属性信息\n",
    "print(\"\\n{}\".format(optimizer.defaults))\n",
    "print(\"\\n{}\".format(optimizer.state))\n",
    "print(\"\\n{}\".format(optimizer.param_groups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.9.3 输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 进行更新前的数据，梯度\n",
    "The data of weight before step:\n",
    "tensor([[-0.3077, -0.1808],\n",
    "        [-0.7462, -1.5556]])\n",
    "The grad of weight before step:\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]])\n",
    "# 进行更新后的数据，梯度\n",
    "The data of weight after step:\n",
    "tensor([[-0.4077, -0.2808],\n",
    "        [-0.8462, -1.6556]])\n",
    "The grad of weight after step:\n",
    "tensor([[1., 1.],\n",
    "        [1., 1.]])\n",
    "# 进行梯度清零的梯度\n",
    "The grad of weight after optimizer.zero_grad():\n",
    "tensor([[0., 0.],\n",
    "        [0., 0.]])\n",
    "# 输出信息\n",
    "optimizer.params_group is \n",
    "[{'params': [tensor([[-0.4077, -0.2808],\n",
    "        [-0.8462, -1.6556]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}]\n",
    "\n",
    "# 证明了优化器的和weight的储存是在一个地方，Python基于值管理\n",
    "weight in optimizer:1841923407424\n",
    "weight in weight:1841923407424\n",
    "    \n",
    "# 输出参数\n",
    "optimizer.param_groups is\n",
    "[{'params': [tensor([[-0.4077, -0.2808],\n",
    "        [-0.8462, -1.6556]], requires_grad=True)], 'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}, {'params': [tensor([[ 0.4539, -2.1901, -0.6662],\n",
    "        [ 0.6630, -1.5178, -0.8708],\n",
    "        [-2.0222,  1.4573,  0.8657]], requires_grad=True)], 'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0}]\n",
    "\n",
    "# 进行更新前的参数查看，用state_dict\n",
    "state_dict before step:\n",
    " {'state': {0: {'momentum_buffer': tensor([[1., 1.],\n",
    "        [1., 1.]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
    "# 进行更新后的参数查看，用state_dict\n",
    "state_dict after step:\n",
    " {'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
    "\n",
    "# 存储信息完毕\n",
    "----------done-----------\n",
    "# 加载参数信息成功\n",
    "load state_dict successfully\n",
    "# 加载参数信息\n",
    "{'state': {0: {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}}, 'param_groups': [{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [0]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [1]}]}\n",
    "\n",
    "# defaults的属性输出\n",
    "{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False}\n",
    "\n",
    "# state属性输出\n",
    "defaultdict(<class 'dict'>, {tensor([[-1.3031, -1.1761],\n",
    "        [-1.7415, -2.5510]], requires_grad=True): {'momentum_buffer': tensor([[0.0052, 0.0052],\n",
    "        [0.0052, 0.0052]])}})\n",
    "\n",
    "# param_groups属性输出\n",
    "[{'lr': 0.1, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [tensor([[-1.3031, -1.1761],\n",
    "        [-1.7415, -2.5510]], requires_grad=True)]}, {'lr': 0.0001, 'nesterov': True, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'params': [tensor([[ 0.4539, -2.1901, -0.6662],\n",
    "        [ 0.6630, -1.5178, -0.8708],\n",
    "        [-2.0222,  1.4573,  0.8657]], requires_grad=True)]}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意：**\n",
    "\n",
    "1. 每个优化器都是一个类，我们一定要进行实例化才能使用，比如下方实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Moddule):\n",
    "\n",
    "net = Net()\n",
    "optim = torch.optim.SGD(net.parameters(),lr=lr)\n",
    "optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer在一个神经网络的epoch中需要实现下面两个步骤：\n",
    "\n",
    "1. 梯度置零\n",
    "2. 梯度更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(net.parameters(), lr=1e-5)\n",
    "for epoch in range(EPOCH):\n",
    "\t...\n",
    "\toptimizer.zero_grad()  #梯度置零\n",
    "\tloss = ...             #计算loss\n",
    "\tloss.backward()        #BP反向传播\n",
    "\toptimizer.step()       #梯度更新"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给网络不同的层赋予不同的优化器参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "net = resnet18()\n",
    "\n",
    "optimizer = optim.SGD([\n",
    "    {'params':net.fc.parameters()},#fc的lr使用默认的1e-5\n",
    "    {'params':net.layer4[0].conv1.parameters(),'lr':1e-2}],lr=1e-5)\n",
    "\n",
    "# 可以使用param_groups查看属性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**注意:**\n",
    "\n",
    "优化器的选择是需要根据模型进行改变的，不存在绝对的好坏之分，我们需要多进行一些测试。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "h_pytorch",
   "language": "python",
   "name": "h_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
